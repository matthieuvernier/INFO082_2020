{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - Práctica\n",
    "\n",
    "- Modelos de lenguaje\n",
    "- Word embedding\n",
    "- Clasificación de textos (Bag-of-words/TFIDF vs. Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets brutos\n",
    "\n",
    "Tenemos a nuestra disposición datasets de dos medios:\n",
    "\n",
    "- **CNN Chile** (https://www.cnnchile.com/): Es un medio de prensa ubicado en Chile. El dueño es *WarnerMedia News & Sports* (un conglomerado de multinacionales de Estados Unidos). \n",
    "\n",
    "- **Cadena SER (España)** (https://cadenaser.com/): Es un medio de prensa ubicado en España. Es dueño es el *Grupo Prisa* (un grupo de empresas de comunicación de España)\n",
    "\n",
    "\n",
    "En su forma bruta, ambos datasets toman la forma de archivo CSV con la estructura siguiente:\n",
    "- ID, country, media_outlet, url, title, body, date\n",
    "\n",
    "El dataset **CNN Chile** contiene 16.472 noticias.\n",
    "\n",
    "El dataset **Cadena SER** contiene 14.378 noticias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparación del dataset CNN Chile\n",
    "\n",
    "A partir del dataset CNN Chile bruto queremos extraer la categoría de la noticia a partir de la columna URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_CSV=\"datasets/CNNCHILE_RAW.csv\"\n",
    "\n",
    "df = pd.read_csv(DATASET_CSV,sep=',',error_bad_lines=False)\n",
    "df = df.drop(['Unnamed: 0'], axis = 1) # Para suprimir la columna ID\n",
    "df['date'] = pd.to_datetime(df['date']) # Para convertir la columna date en formato datetime\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url = df['url'][0]\n",
    "\n",
    "obj = re.findall('(\\w+)://([\\w\\-\\.]+)/([\\w\\-]+).([\\w\\-]+)', url) \n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    url=row['url']\n",
    "    obj = re.findall('(\\w+)://([\\w\\-\\.]+)/([\\w\\-]+).([\\w\\-]+)', url) \n",
    "    \n",
    "    category=obj[0][2]\n",
    "    \n",
    "    df.loc[index,'category'] = category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Cuáles son las categorias del medio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasql import sqldf\n",
    "\n",
    "q=\"\"\"SELECT DISTINCT category FROM df;\"\"\"\n",
    "result=sqldf(q)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Cuántas noticias hay por cada categoría?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"\"\"SELECT category, count(*) FROM df GROUP BY category ORDER BY count(*) DESC;\"\"\"\n",
    "result=sqldf(q)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guardamos solamente las categorias que tienen más de 2000 noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"\"\"SELECT * FROM df WHERE category IN ('pais','deportes','tendencias','tecnologias','cultura','economia','mundo');\"\"\"\n",
    "df_CNN=sqldf(q)\n",
    "df_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"\"\"SELECT * FROM df_CNN WHERE length(body)>5\"\"\"\n",
    "df_CNN=sqldf(q)\n",
    "df_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelos de lenguaje: CNN Chile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_deportes = df_CNN[df_CNN['category']=='deportes']\n",
    "\n",
    "sentences=[]\n",
    "\n",
    "for index, row in df_CNN_deportes.iterrows():\n",
    "    print(index)\n",
    "    text=row['body']\n",
    "    if (text is not None):\n",
    "        doc=nlp(text)\n",
    "        \n",
    "        sentence=[]\n",
    "        for token in doc:\n",
    "            sentence.append(token.text)\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for sentence in sentences:\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "        \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict(model[\"la\",\"persona\"]))\n",
    "print(\"\\n\")\n",
    "print(dict(model[\"persona\",\"más\"]))\n",
    "print(\"\\n\")\n",
    "print(dict(model[\"más\",\"importante\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# starting words\n",
    "text = [\"durante\",\"el\"]\n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "  # select a random probability threshold  \n",
    "  r = random.random()\n",
    "  accumulator = .0\n",
    "\n",
    "  for word in model[tuple(text[-2:])].keys():\n",
    "      accumulator += model[tuple(text[-2:])][word]\n",
    "      # select words that are above the probability threshold\n",
    "      if accumulator >= r:\n",
    "          text.append(word)\n",
    "          break\n",
    "\n",
    "  if text[-2:] == [None, None]:\n",
    "      sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word Embedding: CNN Chile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training word2vec\n",
    "\n",
    "model2 = word2vec.Word2Vec(sentences,size=200,hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.similarity('hombre','mujer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.similarity('hombre','deporte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.similarity('mujer','deporte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.most_similar(positive=['mujer'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.most_similar(positive=['hombre'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.most_similar(positive=['mujer','deporte'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.most_similar(positive=['hombre','deporte'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.most_similar(positive=['hombre','deporte'], negative=['mujer'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.most_similar(positive=['mujer','deporte'], negative=['hombre'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_pais = df_CNN[df_CNN['category']=='pais']\n",
    "\n",
    "sentences_pais=[]\n",
    "\n",
    "for index, row in df_CNN_pais.iterrows():\n",
    "    print(index)\n",
    "    text=row['body']\n",
    "    if (text is not None):\n",
    "        doc=nlp(text)\n",
    "        \n",
    "        sentence=[]\n",
    "        for token in doc:\n",
    "            sentence.append(token.text)\n",
    "        sentences_pais.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training word2vec\n",
    "\n",
    "model3 = word2vec.Word2Vec(sentences_pais,size=200,hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.similarity('hombre','mujer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.similarity('hombre','deporte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.similarity('mujer','deporte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.most_similar(positive=['mujer'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.most_similar(positive=['hombre'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.most_similar(positive=['violencia'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.similarity('violencia','policía')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.similarity('violencia','mujer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.similarity('violencia','hombre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.similarity('violencia','policial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.most_similar(positive=['violencia','policía'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.most_similar(positive=['violencia','manifestante'],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.most_similar(positive=['violencia','manifestación'],topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.n_similarity(['violencia','manifestación'],['manifestantes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.n_similarity(['violencia','manifestación'],['policía'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para comparar con un modelo Word2Vec genérico: https://github.com/dccuchile/spanish-word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "wordvectors_file_vec = 'fasttext-sbwc.3.6.e20.vec'\n",
    "cantidad = 20000\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.wv.similarity('hombre','mujer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.wv.similarity('violencia','policía')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GENERAL:\")\n",
    "print(wordvectors.wv.similarity('mujer','poder'))\n",
    "print(wordvectors.wv.similarity('hombre','poder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Clasificación de textos (bag-of-words vs. Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Bag-of-word: Count y TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "stop_words=\"\"\n",
    "\n",
    "# Load Spanish tokenizer, tagger, parser, NER and word vectors\n",
    "parser = Spanish()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_CNN['body'] # the features we want to analyze\n",
    "ylabels = df_CNN['category'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR1 = LogisticRegression()\n",
    "modelLR2 = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe1 = Pipeline([('preprocessing', bow_vector),\n",
    "                 ('regression-ML', modelLR1)])\n",
    "\n",
    "# Create pipeline using Bag of Words + TFIDF\n",
    "pipe2 = Pipeline([('preprocessing', tfidf_vector),\n",
    "                 ('regression-ML', modelLR2)])\n",
    "\n",
    "# model generation\n",
    "pipe1.fit(X_train,y_train)\n",
    "print(\"modelo #1\")\n",
    "pipe2.fit(X_train,y_train)\n",
    "print(\"modelo #2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted1 = pipe1.predict(X_test)\n",
    "predicted2 = pipe2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluación del rendimiento del clasificador\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted1)\n",
    "print(confusion_matrix)\n",
    "#Print de la matriz de confusión\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluación del rendimiento del clasificador\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted2)\n",
    "print(confusion_matrix)\n",
    "#Print de la matriz de confusión\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Doc2Vec (extensión de Word2Vec para representar documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artículo de Doc2Vec (2014): https://cs.stanford.edu/~quocle/paragraph_vector.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_CNN['body'] # the features we want to analyze\n",
    "ylabels = df_CNN['category'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape\n",
    "#y_train[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_index = {'cultura': 1 , 'deportes': 2, 'economia': 3, 'mundo': 4, 'pais': 5, 'tecnologias': 6, 'tendencias':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents=[]\n",
    "test_documents=[]\n",
    "\n",
    "tags_index = {'cultura': 1 , 'deportes': 2, 'economia': 3, 'mundo': 4, 'pais': 5, 'tecnologias': 6, 'tendencias':7}\n",
    "\n",
    "for i in range(0,11065):\n",
    "    index=X_train.index[i]\n",
    "    text = X_train[index]\n",
    "    tag = y_train[index]\n",
    "    \n",
    "    train_documents.append(TaggedDocument(words=tokenize_text(text), tags=[tags_index.get(tag)] ))\n",
    "    \n",
    "for i in range(0,4742):\n",
    "    index=X_test.index[i]\n",
    "    text = X_test[index]\n",
    "    tag = y_test[index]\n",
    "    \n",
    "    test_documents.append(TaggedDocument(words=tokenize_text(text), tags=[tags_index.get(tag)] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model_dbow = Doc2Vec(dm=1, vector_size=200, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)\n",
    "\n",
    "model_dbow.build_vocab([x for x in tqdm(train_documents)])\n",
    "\n",
    "train_documents  = utils.shuffle(train_documents)\n",
    "\n",
    "model_dbow.train(train_documents,total_examples=len(train_documents), epochs=30)\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "\n",
    "model_dbow.save('./Doc2Vec_model.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vector_for_learning(model_dbow, train_documents)\n",
    "y_test, X_test = vector_for_learning(model_dbow, test_documents)\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted3 = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluación del rendimiento del clasificador\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted3)\n",
    "print(confusion_matrix)\n",
    "#Print de la matriz de confusión\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Ideas para prácticar\n",
    "\n",
    "- #1: Preparar el dataset Cadena SER\n",
    "- #2: Comparar los embeddings: CNN Chile vs. Cadena SER vs. General (--> ¿Cómo se podría sistematizar?)\n",
    "- #3: Visualizar la evolución de los embeddings en el tiempo\n",
    "- #4: Optimizar el modelo de clasificación por temática (otros algoritmos, mejores preprocesamientos, etc.)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
