{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamientos básicos y representación vectorial Bag-of-words aplicados al análisis de sentimientos\n",
    "\n",
    "## 1. ¿Qué es el lenguaje humano?\n",
    "\n",
    "- Es un proceso de __comunicación__ entre humanos basado sobre un sistema complejo de signos\n",
    "\n",
    "- Es un proceso de __percepción__ del mundo \n",
    "\n",
    "<i>¿Es posible pensar el mundo fuera del lenguaje? Ver concepto de <u>neolengua</u> de George Orwell en la novela 1984, quién puede manipular el lenguaje, puede manipular los humanos.</i>\n",
    "\n",
    "- El lenguaje humano puede ser __verbal__ o __no verbal__\n",
    "\n",
    "- El lenguaje verbal puede ser __oral__ o __escrito__\n",
    "\n",
    "- Los humanos utilizan el lenguaje para cumplir 6 grandes tipos de función:\n",
    "    1. _expresar su subjetividad o relaciones de poder_ (opiniones, emociones, creencias, etc.). Ej: \"¡Me gusta la música!\", \"soy mejor que tú\", etc.\n",
    "    1. _solicitar que el interlocutor exprese su subjetividad_. Ej: \"¿Qué piensas?\", \"¿te \"gusta el deporte?\", etc.\n",
    "    1. _describir el mundo_. Ej: \"Hay una mesa y 4 sillas\", etc.\n",
    "    1. _activar o mantener la comunicación_. Ej: \"Hola\", \"Allo\", \"mmm\", \"eeeh\"\n",
    "    1. _ponerse de acuerdo sobre el sentido de un signo_. Ej: \"Hace frio significa que la temperatura es bajo 10°c\"\n",
    "    1. _comunicar por el placer de comunicar_. Ej: poesia, juegos de palabras, bromas lingúïsticas, etc.\n",
    "    \n",
    "\n",
    "- En conclusión, el lenguaje humano siempre se inscribe en una __situación de comunicación__, revela __explicatamente__ o __implicitamente__ el objetivo de comunicacion un locutor, dentro de un contexto particular. \n",
    "\n",
    "Ejemplo: <code>\"¿Viste como llueve?\"</code>\n",
    "\n",
    "Tal vez quiere decir en realidad: \"¡deberías tomar tu paragua!\"\n",
    "\n",
    "- El lenguaje humano tiene distinto nivel de interpretación:\n",
    "    - nivel __lexico-semántico__ : significado las palabras utilizadas.\n",
    "    - nivel __pragmático-discursivo__ : significado de estas palabras en su contexto.\n",
    "\n",
    "<code>\"Hemos realizados intervenciones quirúrgicas en afganistán\"</code>\n",
    "\n",
    "Busca significar que las intervenciones fueron las más limpias y precisas posibles, pero tambien busca ocultar que se trata de bombardeos y de violencia.\n",
    "       \n",
    "\n",
    "## 2. ¿Qué el Tratamiento Automático del Lenguaje (o _NLP_)?\n",
    "\n",
    "- Es una sub-disciplina de la Informática y de la Inteligencia Artificial que busca dotar los computadores de capacidad para entender, traducir y generar lenguaje humano a través de algoritmos y datos.\n",
    "\n",
    "- Es una disciplina antigua pero creciente dado el desarrollo de la comunicación en Internet y el desarrollo de las técnicas de Machine Learning/Deep Learning.\n",
    "\n",
    "- Problemas particulares del TAL en comparación con otras areas: __datos no estructurados__, __muchas(!!!) ambiguedades en los datos__, __problemas de grandes dimensiones__ (muchas variables posibles).\n",
    "\n",
    "- Tareas clásicas: Traducción Automática, Question-Answering, Análisis de opiniones y sentimientos, Extracción de información, Análisis del discurso, etc.\n",
    "\n",
    "- Tareas clásicas para el lenguaje oral: reconocimiento de las palabras, reconocimiento del locutor, etc.\n",
    "\n",
    "- Tarea para el lenguaje no verbal: reconocimiento des las emociones, reconocimiento de gestos, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamientos básicos con spaCy\n",
    "\n",
    "### 3.1 Tokenización, Stop-Words y Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargaremos la librería de NLP spaCy (https://spacy.io/) y los modelos para procesar textos en español.\n",
    "\n",
    "<code>pip3 install -U spacy</code>\n",
    "\n",
    "<code>python3 -m spacy download es_core_news_sm</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Donald Trump es el presidente de Estados Unidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pipeline.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald False Donald\n",
      "Trump False Trump\n",
      "es True ser\n",
      "el True el\n",
      "presidente False presidente\n",
      "de True de\n",
      "Estados True Estados\n",
      "Unidos False Unidos\n",
      ". False .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.is_stop, token.lemma_)\n",
    "    #Stop word: \"palabras gramaticales que no llevan un sentido importante\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El concepto de 'stop words' no tiene una definición objetiva. Una palabra 'stop words' depende mucho del caso de uso. Habitualmente se trata de una lista de palabras gramaticales tales como \"es\", \"el\", \"la\", \"quiere\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [u'presidente']\n",
    "for stopword in my_stop_words:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "    \n",
    "my_non_stop_words = [u'Estados']\n",
    "for nonstopword in my_non_stop_words:\n",
    "    lexeme = nlp.vocab[nonstopword]\n",
    "    lexeme.is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald False Donald\n",
      "Trump False Trump\n",
      "es True ser\n",
      "el True el\n",
      "presidente True presidente\n",
      "de True de\n",
      "Estados False Estados\n",
      "Unidos False Unidos\n",
      ". False .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.is_stop, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Clasificación de la categoría gramatical de las palabras (Part Of Speach tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald PROPN\n",
      "Trump PROPN\n",
      "es AUX\n",
      "el DET\n",
      "presidente NOUN\n",
      "de ADP\n",
      "Estados PROPN\n",
      "Unidos PROPN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Reconocimiento de los nombres de entidades (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon 0 6 PER\n",
      "America del Sur 45 60 LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Amazon tiene oficinas en todos los paises de America del Sur.')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, spaCy utiliza define varios tipos de entidades entre los cuales:\n",
    "- PERSON: personas\n",
    "- ORG: organizaciones, empresas, instituciones, etc.\n",
    "- GPE: paises, ciudades, regiones.\n",
    "- LOC: lugares geografícos que no son paises, ciudades o regiones.\n",
    "- PRODUCT: productos\n",
    "- EVENT: eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " tiene oficinas en todos los paises de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    America del Sur\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otros preprocesamientos posibles en el paquete spaCy: https://spacy.io/usage/linguistic-features\n",
    "- Dependency parsing\n",
    "- Sentence segmentation\n",
    "- Rule-based matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Representación vectorial del lenguaje: Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Modelo _Bag of words_\n",
    "\n",
    "Avanzamos hacia la parte de aprendizaje automático del análisis de texto. Empezaremos a jugar un poco menos con palabras y un poco más con números. Representaremos los textos como __vectores__.\n",
    "\n",
    "Los algoritmos de aprendizaje automático utilizan estos vectores en particular para hacer predicciones (algoritmos de aprendizaje supervisado) o agrupamiento (no supervisado).\n",
    "\n",
    "La manera la más clásica de representar un texto como un vector es el modelo __Bag of Words__.\n",
    "\n",
    "Empecemos con dos frases de ejemplo:\n",
    "\n",
    "F1:<code>El gato juega con el perro</code>,\n",
    "F2:<code>El perro duerme</code>\n",
    "    \n",
    "Si aplicamos los preprocesos que vimos antes, llegamos a:\n",
    "\n",
    "F1:<code>gato jugar perro</code>,\n",
    "F2:<code>perro dormir</code>\n",
    "\n",
    "Si queremos representar esto como un vector, necesitaríamos primero construir nuestro vocabulario, que serían las palabras únicas que se encuentran en las oraciones. \n",
    "\n",
    "<code>Vocab = ['gato', 'jugar', 'dormir', 'perro']</code>\n",
    "\n",
    "Y luego representar las frases con la frecuencia de aparición de cada palabra:\n",
    "\n",
    "F1:<code>[1,1,0,1]</code>\n",
    "F2:<code>[0,0,1,1]</code>\n",
    "\n",
    "##### Limitaciones del modelo _Bag of words_\n",
    "\n",
    "Como pueden darse cuenta, en el modelo Bag of words, se pierde el orden de las palabras y entonces se pierde parte del sentido del texto. Sin embargo, para muchas tareas de clasificación automática el modelo Bag of words es suficiente (ej: detección de spam).\n",
    "\n",
    "Otra limitación de lo visto hasta ahora es que estamos suponiendo que cada palabra tiene la misma importancia para revelar el sentido del texto. Detallamos esta idea en la sección siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Un primer nivel de análisis: la ponderación de la importancia de las palabras (ej. TF-IDF)\n",
    "\n",
    "TF-IDF es la abreviatura de _Term Frequency_ (frecuencia de término) - _Inverse Document Frequency_ (frecuencia de documento inversa). Ampliamente utilizado en los motores de búsqueda para encontrar documentos relevantes basados en una consulta, es un enfoque bastante intuitivo para convertir nuestras frases en vectores.\n",
    "\n",
    "Como su nombre indica, TF-IDF trata de combinar dos tipos diferentes de información:\n",
    "- la frecuencia de término (TF) es el número de veces que una palabra aparece en un documento dividido por el número de palabras en el texto. Mide la importancia local del término en el texto.\n",
    "\n",
    "- IDF es la fracción inversa a escala logarítmica de los documentos que contienen la palabra. \n",
    "IDF(t) = log_e (total number of documents / number of documents with term t in it)\n",
    "\n",
    "\n",
    "TF-IDF es simplemente el producto de estos dos factores - TF e IDF. Juntos, encapsulan más información en la representación vectorial, en lugar de limitarse a utilizar el recuento de palabras como en la representación vectorial de la bolsa de palabras. TF-IDF hace que las palabras raras sean más relevantes para representar el sentido del texto.\n",
    "\n",
    "Si tomamos nuestras frases de ejemplo, tendriamos los vectores siguientes:\n",
    "F1:<code>[0.1,0.1,0,0]</code>\n",
    "F2:<code>[0,0,0.15,0]</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Un ejemplo real de representación vectorial de un dataset de documentos\n",
    "\n",
    "#### 4.3.1 Dataset (o _Corpus_)\n",
    "\n",
    "Utilizaremos un dataset de textos en español que corresponde a una muestra de 1.000 noticias publicadas en 2018 por el medio La Tercera (http://www.latercera.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-04 07:00:44</td>\n",
       "      <td>latercera</td>\n",
       "      <td>Bachelet propone establecer por ley que \"no h...</td>\n",
       "      <td>Tras visitar a la selección nacional en Juan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-04 08:49:38</td>\n",
       "      <td>latercera</td>\n",
       "      <td>Adriana Delpiano por Reforma educacional: \"De...</td>\n",
       "      <td>Hoy ingresa a la Cámara de Diputados el proye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-04 09:40:11</td>\n",
       "      <td>latercera</td>\n",
       "      <td>Cómo será la aventura de la sonda Juno en Júp...</td>\n",
       "      <td>Cinco años después de su lanzamiento, la sond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-04 13:25:15</td>\n",
       "      <td>latercera</td>\n",
       "      <td>Se derrumba el reencuentro de las Spice Girls...</td>\n",
       "      <td>En marzo de este año se anunciaba de un posib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-03 21:18:00</td>\n",
       "      <td>latercera</td>\n",
       "      <td>Presidenta Bachelet: \"Que quede establecido p...</td>\n",
       "      <td>La Presidenta Michelle Bachelet realizó este ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0            1  \\\n",
       "0 2016-07-04 07:00:44   latercera    \n",
       "1 2016-07-04 08:49:38   latercera    \n",
       "2 2016-07-04 09:40:11   latercera    \n",
       "3 2016-07-04 13:25:15   latercera    \n",
       "4 2016-07-03 21:18:00   latercera    \n",
       "\n",
       "                                                   2  \\\n",
       "0   Bachelet propone establecer por ley que \"no h...   \n",
       "1   Adriana Delpiano por Reforma educacional: \"De...   \n",
       "2   Cómo será la aventura de la sonda Juno en Júp...   \n",
       "3   Se derrumba el reencuentro de las Spice Girls...   \n",
       "4   Presidenta Bachelet: \"Que quede establecido p...   \n",
       "\n",
       "                                                   3  \n",
       "0   Tras visitar a la selección nacional en Juan ...  \n",
       "1   Hoy ingresa a la Cámara de Diputados el proye...  \n",
       "2   Cinco años después de su lanzamiento, la sond...  \n",
       "3   En marzo de este año se anunciaba de un posib...  \n",
       "4   La Presidenta Michelle Bachelet realizó este ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_CSV=\"sophia_latercera-1000.csv\"\n",
    "\n",
    "df = pd.read_csv(DATASET_CSV,sep='|',error_bad_lines=False,header=None)\n",
    "df[0] = pd.to_datetime(df[0])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>noticia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-04 07:00:44</td>\n",
       "      <td>Tras visitar a la selección nacional en Juan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-04 08:49:38</td>\n",
       "      <td>Hoy ingresa a la Cámara de Diputados el proye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-04 09:40:11</td>\n",
       "      <td>Cinco años después de su lanzamiento, la sond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-04 13:25:15</td>\n",
       "      <td>En marzo de este año se anunciaba de un posib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-03 21:18:00</td>\n",
       "      <td>La Presidenta Michelle Bachelet realizó este ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fecha                                            noticia\n",
       "0 2016-07-04 07:00:44   Tras visitar a la selección nacional en Juan ...\n",
       "1 2016-07-04 08:49:38   Hoy ingresa a la Cámara de Diputados el proye...\n",
       "2 2016-07-04 09:40:11   Cinco años después de su lanzamiento, la sond...\n",
       "3 2016-07-04 13:25:15   En marzo de este año se anunciaba de un posib...\n",
       "4 2016-07-03 21:18:00   La Presidenta Michelle Bachelet realizó este ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=df[[0,3]]\n",
    "texts.columns = ['fecha', 'noticia']\n",
    "\n",
    "texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>noticia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>2016-03-05 01:18:32</td>\n",
       "      <td>El gobierno chino se ha fijado un objetivo de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>2016-03-05 03:20:09</td>\n",
       "      <td>Por “atentado en contra de la autoridad” fue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>2016-03-05 04:40:10</td>\n",
       "      <td>La semana pasada tres millones de estudiantes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>2016-03-05 05:40:11</td>\n",
       "      <td>El virus del Zika presente en América Latina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>2016-03-05 10:11:37</td>\n",
       "      <td>“Es como si la gente hubiera levantado una ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  fecha                                            noticia\n",
       "934 2016-03-05 01:18:32   El gobierno chino se ha fijado un objetivo de...\n",
       "935 2016-03-05 03:20:09   Por “atentado en contra de la autoridad” fue ...\n",
       "936 2016-03-05 04:40:10   La semana pasada tres millones de estudiantes...\n",
       "937 2016-03-05 05:40:11   El virus del Zika presente en América Latina ...\n",
       "938 2016-03-05 10:11:37   “Es como si la gente hubiera levantado una ro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=texts.sort_values(by=['fecha'])\n",
    "texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nada de contento se mostró Esteban Paredes con la llamativa indumentaria de Colo Colo. El conjunto de José Luis Sierra, que ayer perdió 1-0 ante Unión Española en el Monumental, vistió la nueva camiseta roja, algo que molestó al delantero albo, quien hizo sus descargos a través de redes sociales. “Por favor cambiemos y respetemos los colores de mi amado Colo Colo”, publicó Paredes, quien no estuvo presente en el encuentro frente a los hispanos, en su cuenta de Twitter.   La nueva camiseta de los albos, con la que ha disputado los amistosos, será utilizada en los encuentros de Copa Chile, que arranca la semana que viene.  Por favor cambiemos y respetemos los colores de mi amado colo- colo — esteban paredes (@estebanparedesQ) 3 de julio de 2016\n",
      "2016-07-04 05:45:15\n"
     ]
    }
   ],
   "source": [
    "for index,row in texts.iterrows():\n",
    "    if index==6:\n",
    "        print(row['noticia'])\n",
    "        print(row['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha\n",
       "2016-03     50\n",
       "2016-04     80\n",
       "2016-05    141\n",
       "2016-06     81\n",
       "2016-07    104\n",
       "2016-08     51\n",
       "2016-09     88\n",
       "2016-10    181\n",
       "2016-11    201\n",
       "2016-12     23\n",
       "Freq: M, Name: noticia, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_texts_perMonth = texts.groupby(texts.fecha.dt.to_period(\"M\")).count()\n",
    "result=n_texts_perMonth['noticia']\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = result.plot.bar(x='fecha', y='noticia', rot=90, color=(0.2, 0.4, 0.6, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Preprocesamientos básicos (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>noticia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>2016-03-05 01:18:32</td>\n",
       "      <td>El gobierno chino se ha fijado un objetivo de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>2016-03-05 03:20:09</td>\n",
       "      <td>Por “atentado en contra de la autoridad” fue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>2016-03-05 04:40:10</td>\n",
       "      <td>La semana pasada tres millones de estudiantes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>2016-03-05 05:40:11</td>\n",
       "      <td>El virus del Zika presente en América Latina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>2016-03-05 10:11:37</td>\n",
       "      <td>“Es como si la gente hubiera levantado una ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  fecha                                            noticia\n",
       "934 2016-03-05 01:18:32   El gobierno chino se ha fijado un objetivo de...\n",
       "935 2016-03-05 03:20:09   Por “atentado en contra de la autoridad” fue ...\n",
       "936 2016-03-05 04:40:10   La semana pasada tres millones de estudiantes...\n",
       "937 2016-03-05 05:40:11   El virus del Zika presente en América Latina ...\n",
       "938 2016-03-05 10:11:37   “Es como si la gente hubiera levantado una ro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=texts.head(5)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ', 'gobernar', 'chino', 'fijar', 'objetivar', 'crecimiento', 'pib', 'y', 'ciento', 'frente', 'ciento', 'a', 'luz', 'ralentización', 'sufrir', 'economía', 'mundial', '\\xa0 ', 'meter', 'figurar', 'informar', 'gubernamental', 'leer', 'ministro', 'li', 'keqiang', 'asamblea', 'nacional', 'popular', 'anp', 'legislativo', 'establecer', 'chino', 'año', 'lograr', 'cumplir', 'objetivar', 'crecimiento', '10', 'pib', 'aumentar', 'ciento', '\\xa0 ', 'informar', 'fijo', 'indicador', 'inflación', 'mantener', 'tornar', 'ciento', 'ejercicio', 'anterior', 'creación', 'millón', 'puesto', 'urbano', 'frente', 'a', 'millón', 'generar', 'y', 'aumentar', 'presupuestar', 'defensa', 'ciento', 'año', '\\xa0 ', 'perspectivo', 'necesidad', 'avanzar', 'reformar', 'estructural', 'y', 'ayudar', 'a', 'guiar', 'expectativa', 'mercar', 'y', 'mantenerlas', 'estable', 'señalar', 'informar', 'proyectar', 'crecimiento', 'pib', 'ciento', 'lustrar', 'aplicar', 'xiii', 'plan', 'quinquenal', '\\xa0 ', 'cifrar', 'contrastar', 'década', 'pasar', 'chino', 'lograr', 'alzar', 'pib', 'anual', 'ciento', 'pekín', 'afirmar', 'a', 'medir', 'desarrollar', 'país', 'complicar', 'mantener', 'alto', 'cifrar', 'crecimiento', '\\xa0 ', 'aumentar', 'punto', 'porcentual', 'pib', 'equivaler', 'a', 'año', 'o', 'defender', 'informar', 'repasar', 'régimen', 'comunista', 'y', 'marcar', 'ruta', 'a', 'seguir', 'año', 'sesión', 'anual', 'anp', 'debatir', 'plan', 'quinquenal', 'y', 'aprobar', 'a', 'mediar', 'marzo'], [' ', 'atentar', 'autoridad', 'formalizar', 'mediodía', 'mujer', 'identificar', 'inicial', 'j.', 'y.', 'm.', 'detenido', 'jueves', 'comuna', 'tirúa', 'región', 'biobío', 'protagonizar', 'incidente', 'patrullar', 'carabinero', 'fundir', 'wagemann', 'procedimiento', 'judicial', 'efectuar', 'juzgar', 'garantir', 'cañete', 'participación', 'hecho', 'grave', 'concertación', 'cometer', 'tipo', 'delito', 'etapa', 'investigativa', 'fiscal', 'juan', 'yáñez', 'persecutor', 'dedicación', 'exclusivo', 'investigar', 'hecho', 'violencia', 'rural', 'provincia', 'arauco', 'imputar', 'medir', 'cautelar', 'firmar', 'semanal', 'ministerio', 'público', 'solicitar', 'arrestar', 'domiciliario', 'nocturno', 'tribunal', 'rechazar', 'plazo', 'investigación', 'mesar', 'patrullaje', 'detención', 'producir', 'patrullaje', 'realizar', 'carabinero', 'emboscar', 'miércoles', 'zona', 'sufrir', 'convoy', 'camión', 'forestal', 'vehículo', 'resultar', 'quemar', 'producto', 'atacar', 'incendiario', 'efectuar', 'encapuchar', 'fiscal', 'yáñez', 'detención', 'efectuar', 'jueves', 'carabinero', 'encontrar', 'camioneta', 'sujeto', 'comenzar', 'a', 'disparar', 'lograr', 'a', 'fugar', 'y', 'interceptar', 'policía', 'camioneta', 'lograr', 'detenido', 'parapetar', 'y', 'comenzar', 'disparo', 'personar', 'vehículo', 'arrancar', 'lograr', 'detención', 'conductor', 'y', 'haber', 'efectuar', 'disparo', 'anoche', 'sujetar', 'hombre', 'inicial', 'h.', 'p.', 'p.', 'permanecer', 'internar', 'hospital', 'curanilahue', 'presentar', 'lesionar', 'formalizar', 'portar', 'ilegal', 'armar', 'y', 'munición', 'receptación', 'escopeta', 'hechizo', 'y', 'atentar', 'autoridad', 'prisión', 'preventivo'], [' ', 'semana', 'millón', 'estudiante', 'ingresar', 'a', 'clase', 'comenzar', 'ciclar', 'escolar', 'mil', 'niño', 'y', 'joven', 'optar', 'caminar', 'estudiar', 'y', 'examen', 'libre', '\\xa0 ', 'casar', 'mariano', 'parra', 'colegiar', '3', 'sentir', 'escuela', 'y', 'querer', 'formar', 'gracia', 'a', 'gestión', 'padre', 'comenzar', 'a', 'estudiar', 'a', 'profesor', 'particular', 'temario', 'entregar', 'colegiar', 'examinador', 'prueba', 'libre', 'lograr', 'terminar', 'etapa', 'escolar', 'y', 'entrar', 'a', 'estudiar', 'ingeniería', 'informático', 'duoc', 'formar', 'programar', 'estudiante', 'inscribirse', 'ministerio', 'educación', 'mineduc', 'autorizar', 'casar', 'menor', 'edad', 'cartero', 'asignar', 'colegiar', 'examinador', 'preparar', 'prueba', '\\xa0 ', 'jefe', 'división', 'educación', 'mineduc', 'gonzalo', 'muñoz', '  ', 'establecimiento', 'educacional', 'designar', 'escoger', 'preferentemente', 'resultar', 'simce', '\\xa0 ', 'casar', 'mayor', 'edad', 'autorización', 'y', 'temario', 'correr', 'mineduc', 'a', 'muñoz', 'examen', '  ', 'constituir', 'excepcionalidad', 'sistema', 'educacional', 'y', 'opción', 'culminar', 'estudio', 'año', 'mineduc', 'cursar', 'autorización', '  ', 'corresponder', 'a', 'menor', 'y', 'a', 'mayor', 'año', 'llamativo', 'casar', '  ', 'mayor', 'poder', 'prueba', 'aprobar', 'nivelar', '°', 'y', '°', '°', 'básico', 'infografía', 'a', 'contar', 'cifrar', 'muñoz', 'estadístico', 'específico', 'efectivamente', 'rendir', 'examen', 'libre', 'y', 'estimar', 'mitad', 'terminar', 'realizar', '  ', 'trámite', '\\xa0 ', 'mariano', 'benjamín', 'herrero', 'decidir', 'asistir', 'colegiar', 'problema', 'personal', 'y', 'prueba', 'año', 'aprobar', '°', 'y', '°', 'mineduc', 'facilitar', 'libro', 'educarse', 'y', 'prueba', 'libre', 'colegiar', 'examinador', 'sistema', 'favorecer', 'estudio', 'aprender', 'a', 'responsable', 'malo', 'estudiar', 'y', 'tomar', 'atención', 'clase', 'situación', 'cambiar', 'recalcar', 'pesar', 'a', 'fácil', 'estudiar', 'y', 'esperar', 'volver', 'colegiar'], [' ', 'virus', 'zika', 'presentar', 'américa', 'latino', '  ', 'atacar', 'y', 'destruir', 'célula', 'cerebral', 'humano', 'desarrollar', 'feto', '  ', 'revelar', 'estudiar', 'publicar', 'viernes', 'basar', 'a', 'observación', 'laboratorio', '\\xa0 ', 'estudiar', 'probar', 'experimental', 'vínculo', 'biológico', 'virus', 'transmitir', 'mosquito', 'y', 'drástico', 'incrementar', 'caso', '  ', 'microcefalia', 'severo', 'malformación', 'cerebro', 'y', 'cráneo', 'recién', '  ', 'nacido', '\\xa0 ', 'vínculo', 'comprobar', 'circunstancial', 'explicar', 'guo', 'li', 'ming', 'profesor', 'neurología', 'instituto', 'ingeniería', 'celular', 'instituto', '  ', 'johns', 'hopkins', 'maryland', 'unir', 'codirigir', '  ', 'investigación', '\\xa0 ', 'estudio', 'feto', 'y', 'bebé', 'cerebro', 'reducir', 'y', 'microcefalia', 'zona', '  ', 'afectar', 'virus', 'zika', 'hallar', 'anomalía', 'córtex', 'y', 'virus', '  ', 'tejido', 'fetal', 'indicar', 'estudiar', '\\xa0 ', 'experiencia', 'laboratorio', 'científico', 'exponer', 'tipo', 'célula', 'humano', 'virus', 'zika', '\\xa0 ', 'conocer', 'célula', 'progenitor', 'neuronal', 'humano', 'hnpcs', 'crucial', 'desarrollar', 'córtex', 'o', 'capar', 'superficial', 'cerebro', '  ', 'feto', '\\xa0 ', 'dañar', 'a', 'célula', 'desarrollar', 'neurona', 'maduro', 'coherente', 'trastorno', 'causar', 'microcefalia', '\\xa0 ', 'tipo', 'célula', 'célula', 'madre', 'y', 'neurona', '\\xa0 ', 'prever', 'virus', 'zika', 'atacar', 'a', 'hnpcs', '  ', 'exposición', 'resultar', 'infectar', 'y', 'morir', '\\xa0 ', 'célula', 'infectar', 'comenzar', 'a', 'replicar', 'copiar', '  ', 'virus', '\\xa0 ', 'gen', 'necesario', 'luchar', 'virus', 'activar', '  ', 'altamente', 'inusual', '\\xa0 ', 'comparación', 'tipo', 'célula', 'humano', 'resultar', '  ', 'relativamente', 'ileso', '\\xa0 ', 'resultar', 'demostrar', 'claramente', 'prueba', 'laboratorio', '  ', 'zika', 'infectar', 'directamente', 'y', 'eficacia', 'a', 'hnpcs', 'concluir', '  ', 'estudiar', '\\xa0 ', 'resultar', 'significativo', 'célula', 'formar', 'córtex', '  ', 'potencialmente', 'vulnerable', 'virus', 'agregar', 'ming', '\\xa0 ', 'resultar', 'publicar', 'revistar', 'cell', 'stem', 'cell', 'ayudar', 'a', '  ', 'identificar', 'medicamento', 'capaz', 'proteger', 'a', 'célula', 'vulnerable', 'o', '  ', 'reducir', 'infección', 'producir', '\\xa0 ', 'zika', 'peligroso', 'resfriar', 'o', 'casar', 'gripe', '  ', 'a', 'presentar', 'síntoma', '\\xa0 ', 'sospechar', 'brotar', 'recentar', 'virus', '  ', 'extender', 'rápidamente', 'y', 'presentar', 'decena', 'país', '  ', 'organización', 'mundial', 'salud', 'oms', 'causar', 'microcefalia', 'y', '  ', 'complicación', 'severo', '  '], [' ', 'gente', 'haber', 'levantar', 'roca', 'y', 'mirar', 'animador', 'bromear', 'director', 'zumbástico', 'studios', 'álvaro', 'ceppi', 'a', 'extremo', 'atención', 'gozar', 'animación', 'chileno', 'historia', 'osar', 'ganar', 'oscar', 'extremo', 'mediatización', 'llevar', 'realizar', 'silenciar', 'animador', 'país', 'año', 'y', 'volver', 'objetar', 'interés', 'chileno', 'plataforma', 'extranjero', 'cartoon', 'network', 'semana', 'compañía', 'anunciar', 'convocatorio', 'proyecto', 'animar', 'chileno', 'presentarse', 'abril', 'a', 'concursar', 'convertirse', 'futuro', 'serie', 'canal', 'animación', 'grande', 'mundo', 'confianza', 'efecto', 'oscar', 'erwin', '  ', 'gómez', 'director', 'fundación', 'chilemonos', 'agrupar', 'a', 'animación', 'independiente', 'oscar', 'chile', 'animación', 'ver', 'venir', 'preguntar', '\\xa0 ', 'historia', 'comenzar', 'animación', 'chile', 'remontar', 'año', 'estrenar', 'dibujo', 'película', 'animar', 'chileno', 'proyectar', 'visitar', 'walt', 'disney', 'a', 'país', 'hito', 'pantalla', 'chico', '  ', '  ', 'tevito', 'mascota', 'tvn', 'gobernar', 'salvador', 'allende', 'y', 'angelito', 'canal', 'abrir', 'y', 'cerrar', 'transmisión', 'canal', 'y', 'pasar', 'importante', '  ', 'ogú', 'y', 'mampato', 'rapar', 'nui', 'alejandro', 'rojo', 'considerar', 'filmar', 'animación', 'moderno', 'chile', 'director', 'papelucho', 'y', 'marciano', 'animar', 'y', 'oscar', 'premiar', 'a', 'historia', 'osar', 'aislar', 'acompañar', 'boom', 'productor', 'independiente', '-que', 'sumir', '25-', 'profesionalización', 'disciplinar', 'país', 'comenzar', 'año', 'y', 'impartir', 'casar', 'estudio', 'acreditar', 'casar', 'universidad', '  ', 'américas', 'y', 'universidad', 'producir', 'chile', 'película', 'animar', 'contar', 'dibujo', 'produciéndose', 'largometraje', 'germán', 'acuñar', 'presidente', 'asociación', 'gremial', 'animador', 'animachi', 'responsable', 'nahuel', 'y', 'librar', 'mágico', 'aporte', 'fondo', 'fomentar', 'audiovisual', 'y', 'cntv', 'historia', 'niño', 'chilote', 'hijo', 'pescador', 'miedo', 'mar', 'enfrentar', 'desafío', 'encontrarse', 'padre', 'separar', '  ', 'naufragar', 'tormenta', 'acuñar', '  ', 'cintar', '  ', 'esperar', 'finalizar', 'y', 'obtener', 'millón', 'peso', 'fondo', 'audiovisual', 'consejo', 'nacional', 'cultura', 'aumentar', 'producción', 'película', 'animar', 'chile', 'explicar', 'fondo', 'audiovisual', 'destin', 'ó', 'recurso', 'exclusivo', 'largometraje', 'animar', 'año', 'adjudicar', 'productor', 'zumbástico', 'y', 'diluviar', 'proyectar', 'mangar', 'brothers', 'película', 'ciencia', 'ficción', '3d', 'trabajar', 'a', 'punkrobot', 'aliens', 'querer', 'conquistar', 'planeta', 'fruto', 'y', 'verdura', 'esperar', 'estar', 'listo', 'millón', 'apoyar', 'consejo', 'cultura', 'casar', 'lobo', 'proyectar', 'cristóbal', 'león', 'y', 'joaquín', 'cociña', 'productor', 'diluviar', 'convertir', 'película', 'stop', 'motion', 'hacer', 'chile', 'sellar', 'artístico', 'comercial', 'película', 'inspirar', 'colonia', 'dignidad', 'historia', 'mujer', 'escapar', 'y', 'llegar', 'a', 'casar', 'habitar', 'cerdo', 'explicar', 'cociña', 'jugar', 'estar', 'filmar', 'productor', 'animación', 'imaginario', 'colonia', 'dignidad', 'agregar', 'león', 'llevar', 'entregar', 'a', 'proyectar', 'año', 'y', 'esperar', 'terminarla', 'año', 'más.el', 'panorama', 'completar', 'ojo', 'gato', 'productor', 'atiempo', 'largometraje', '3d', 'ambientar', 'valparaíso', 'y', 'homeless', 'coproducción', 'lunes', 'y', 'fábula', 'dirigir', 'público', 'adulto', 'niño', 'rico', 'soñar', 'vivir', 'vagabundo', 'serie', 'serie', 'tv', 'animar', 'desarrollar', 'chile', 'sumir', 'y', 'título', 'chileno', 'promediar', 'poner', 'a', 'enumerar', 'conocer', 'seguramente', 'recordar', 'pulentos', 'villa', 'dulce', 'diego', 'y', 'glot', 'tortuga', 'tarugo', 'clarita', 'y', 'suerte', 'paso', 'decena', 'producción', 'animar', 'chileno', 'ganar', 'oscar', 'punkrobot', 'contar', 'serie', 'a', 'flippos', 'y', 'aventurar', 'muelín', 'y', 'perlita', 'transmitir', 'filial', 'infantil', 'globo', 'y', 'ambos', 'netflix', 'parecer', 'paso', 'zumbástico', 'fantástico', 'seriar', 'zumbástico', 'studio', 'comprar', 'cartoon', 'network', 'y', 'transmitir', 'cadena', 'país', 'año', 'seguir', 'cartoon', 'network', 'seriar', 'exitoso', 'pasar', 'seriar', 'tvn', '  ', 'resonancia', 'explicar', 'ceppi', '  ', 'casar', 'similar', 'hostal', 'morrison', 'productor', 'chileno', 'pájaro', 'temporada', 'cartoon', 'network', 'transmitir', 'canal', 'argentino', 'pakapaka', 'año', 'seguir', 'chile', 'temporada', 'emitir', '  ', 'canal', 'y', 'chv', 'preguntar', 'a', 'niño', 'argentino', 'conocer', 'hostal', 'morrison', 'conocer', 'preguntar', 'a', 'niño', 'chileno', 'conocer', 'ejemplificar', 'bernardita', 'ojeda', 'director', 'pájaro', 'erwin', 'gómez', 'álvaro', 'ceppi', 'y', 'bernardita', 'ojeda', 'concordar', 'origen', 'fenómeno', 'escaso', 'apoyar', 'canal', 'televisión', 'abrir', 'brindar', 'a', 'tipo', 'producción', 'perpetuar', 'largo', 'año', 'transmisión', 'programar', 'extranjero', 'simpson', 'terminar', 'sepultar', 'franja', 'programación', 'infantil', '  ', 'chespirito', 'haber', 'chileno', 'chavo', 'haber', 'existir', 'tv', 'haber', 'oportunidad', 'lamentar', 'lópez', 'año', 'tvn', 'transmitir', 'serie', 'animar', 'chileno', 'puerto', 'papel', 'stop', 'motion', 'productor', 'zumbástico', 'comenzar', 'a', 'transmitirse', 'semestre', 'seriar', 'matilde', 'niño', 'vacación', 'abuelo', 'pirata', 'cocar', 'mágico', 'a', 'matilde', 'distinto', 'elegir', 'controlar', 'a', 'creación', 'chileno', 'éxito', 'aforar', 'único', 'gente', 'conocer', 'y', 'disfrutar', 'ceppi', 'financiamiento', 'temer', 'complicar', 'a', 'animador', 'soler', 'encontrar', 'presupuestar', 'preproducción', 'proyecto', 'fondo', 'corfo', 'buscar', 'apoyar', 'a', '  ', 'fondo', 'cntv', 'o', 'consejo', 'cultura', 'película', 'animar', 'caro', 'técnico', 'utilizar', 'complejo', 'y', 'softwares', 'costoso', 'recurso', 'apoyar', 'cntv', 'y', 'consejo', 'cultura', 'existir', 'aclarar', 'ceppi.pero', 'apoyar', 'organismo', 'a', 'animación', 'incierto', 'patrocinio', 'cntv', 'a', 'proyecto', 'animación', 'infantil', 'nulo', 'estancar', 'año', 'desarrollar', 'propuesta', 'invertir', 'a', 'fondo', 'corfo', '\\xa0 ', 'profundar', 'error', 'pensar', 'erwin', 'gómez.tras', 'oscar', 'historia', 'osar', 'animador', 'chileno', 'esperar', 'quedar', 'celebración', 'y', 'generar', 'cambio', 'estructural', 'mejorar', 'condicionar', 'y', 'oportunidad', 'producir', 'y', 'mostrar', 'contenido', 'oscar', 'deber', 'marcar', 'comenzar', 'industrialización', 'esperar', 'director', 'chilemonos', 'interés', 'aforar', 'chile', 'necesitar', 'apañar', 'venir', 'grande', 'concluir']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "noticias = []\n",
    "\n",
    "for index,row in dataset.iterrows():\n",
    "    noticia = []\n",
    "    doc=nlp(row['noticia'].lower())\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.is_digit and not token.like_num:\n",
    "            noticia.append(token.lemma_)\n",
    "    noticias.append(noticia)\n",
    "    \n",
    "print(noticias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'semana',\n",
       " 'millón',\n",
       " 'estudiante',\n",
       " 'ingresar',\n",
       " 'a',\n",
       " 'clase',\n",
       " 'comenzar',\n",
       " 'ciclar',\n",
       " 'escolar',\n",
       " 'mil',\n",
       " 'niño',\n",
       " 'y',\n",
       " 'joven',\n",
       " 'optar',\n",
       " 'caminar',\n",
       " 'estudiar',\n",
       " 'y',\n",
       " 'examen',\n",
       " 'libre',\n",
       " '\\xa0 ',\n",
       " 'casar',\n",
       " 'mariano',\n",
       " 'parra',\n",
       " 'colegiar',\n",
       " '3',\n",
       " 'sentir',\n",
       " 'escuela',\n",
       " 'y',\n",
       " 'querer',\n",
       " 'formar',\n",
       " 'gracia',\n",
       " 'a',\n",
       " 'gestión',\n",
       " 'padre',\n",
       " 'comenzar',\n",
       " 'a',\n",
       " 'estudiar',\n",
       " 'a',\n",
       " 'profesor',\n",
       " 'particular',\n",
       " 'temario',\n",
       " 'entregar',\n",
       " 'colegiar',\n",
       " 'examinador',\n",
       " 'prueba',\n",
       " 'libre',\n",
       " 'lograr',\n",
       " 'terminar',\n",
       " 'etapa',\n",
       " 'escolar',\n",
       " 'y',\n",
       " 'entrar',\n",
       " 'a',\n",
       " 'estudiar',\n",
       " 'ingeniería',\n",
       " 'informático',\n",
       " 'duoc',\n",
       " 'formar',\n",
       " 'programar',\n",
       " 'estudiante',\n",
       " 'inscribirse',\n",
       " 'ministerio',\n",
       " 'educación',\n",
       " 'mineduc',\n",
       " 'autorizar',\n",
       " 'casar',\n",
       " 'menor',\n",
       " 'edad',\n",
       " 'cartero',\n",
       " 'asignar',\n",
       " 'colegiar',\n",
       " 'examinador',\n",
       " 'preparar',\n",
       " 'prueba',\n",
       " '\\xa0 ',\n",
       " 'jefe',\n",
       " 'división',\n",
       " 'educación',\n",
       " 'mineduc',\n",
       " 'gonzalo',\n",
       " 'muñoz',\n",
       " '  ',\n",
       " 'establecimiento',\n",
       " 'educacional',\n",
       " 'designar',\n",
       " 'escoger',\n",
       " 'preferentemente',\n",
       " 'resultar',\n",
       " 'simce',\n",
       " '\\xa0 ',\n",
       " 'casar',\n",
       " 'mayor',\n",
       " 'edad',\n",
       " 'autorización',\n",
       " 'y',\n",
       " 'temario',\n",
       " 'correr',\n",
       " 'mineduc',\n",
       " 'a',\n",
       " 'muñoz',\n",
       " 'examen',\n",
       " '  ',\n",
       " 'constituir',\n",
       " 'excepcionalidad',\n",
       " 'sistema',\n",
       " 'educacional',\n",
       " 'y',\n",
       " 'opción',\n",
       " 'culminar',\n",
       " 'estudio',\n",
       " 'año',\n",
       " 'mineduc',\n",
       " 'cursar',\n",
       " 'autorización',\n",
       " '  ',\n",
       " 'corresponder',\n",
       " 'a',\n",
       " 'menor',\n",
       " 'y',\n",
       " 'a',\n",
       " 'mayor',\n",
       " 'año',\n",
       " 'llamativo',\n",
       " 'casar',\n",
       " '  ',\n",
       " 'mayor',\n",
       " 'poder',\n",
       " 'prueba',\n",
       " 'aprobar',\n",
       " 'nivelar',\n",
       " '°',\n",
       " 'y',\n",
       " '°',\n",
       " '°',\n",
       " 'básico',\n",
       " 'infografía',\n",
       " 'a',\n",
       " 'contar',\n",
       " 'cifrar',\n",
       " 'muñoz',\n",
       " 'estadístico',\n",
       " 'específico',\n",
       " 'efectivamente',\n",
       " 'rendir',\n",
       " 'examen',\n",
       " 'libre',\n",
       " 'y',\n",
       " 'estimar',\n",
       " 'mitad',\n",
       " 'terminar',\n",
       " 'realizar',\n",
       " '  ',\n",
       " 'trámite',\n",
       " '\\xa0 ',\n",
       " 'mariano',\n",
       " 'benjamín',\n",
       " 'herrero',\n",
       " 'decidir',\n",
       " 'asistir',\n",
       " 'colegiar',\n",
       " 'problema',\n",
       " 'personal',\n",
       " 'y',\n",
       " 'prueba',\n",
       " 'año',\n",
       " 'aprobar',\n",
       " '°',\n",
       " 'y',\n",
       " '°',\n",
       " 'mineduc',\n",
       " 'facilitar',\n",
       " 'libro',\n",
       " 'educarse',\n",
       " 'y',\n",
       " 'prueba',\n",
       " 'libre',\n",
       " 'colegiar',\n",
       " 'examinador',\n",
       " 'sistema',\n",
       " 'favorecer',\n",
       " 'estudio',\n",
       " 'aprender',\n",
       " 'a',\n",
       " 'responsable',\n",
       " 'malo',\n",
       " 'estudiar',\n",
       " 'y',\n",
       " 'tomar',\n",
       " 'atención',\n",
       " 'clase',\n",
       " 'situación',\n",
       " 'cambiar',\n",
       " 'recalcar',\n",
       " 'pesar',\n",
       " 'a',\n",
       " 'fácil',\n",
       " 'estudiar',\n",
       " 'y',\n",
       " 'esperar',\n",
       " 'volver',\n",
       " 'colegiar']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticias[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Representación vectorial (Gensim): Bag of words (sin ponderación)\n",
    "\n",
    "Gensim: https://radimrehurek.com/gensim/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '10': 1, 'a': 2, 'afirmar': 3, 'alto': 4, 'alzar': 5, 'anp': 6, 'anterior': 7, 'anual': 8, 'aplicar': 9, 'aprobar': 10, 'asamblea': 11, 'aumentar': 12, 'avanzar': 13, 'ayudar': 14, 'año': 15, 'chino': 16, 'ciento': 17, 'cifrar': 18, 'complicar': 19, 'comunista': 20, 'contrastar': 21, 'creación': 22, 'crecimiento': 23, 'cumplir': 24, 'debatir': 25, 'defender': 26, 'defensa': 27, 'desarrollar': 28, 'década': 29, 'economía': 30, 'ejercicio': 31, 'equivaler': 32, 'estable': 33, 'establecer': 34, 'estructural': 35, 'expectativa': 36, 'figurar': 37, 'fijar': 38, 'fijo': 39, 'frente': 40, 'generar': 41, 'gobernar': 42, 'gubernamental': 43, 'guiar': 44, 'indicador': 45, 'inflación': 46, 'informar': 47, 'keqiang': 48, 'leer': 49, 'legislativo': 50, 'li': 51, 'lograr': 52, 'lustrar': 53, 'luz': 54, 'mantener': 55, 'mantenerlas': 56, 'marcar': 57, 'marzo': 58, 'mediar': 59, 'medir': 60, 'mercar': 61, 'meter': 62, 'millón': 63, 'ministro': 64, 'mundial': 65, 'nacional': 66, 'necesidad': 67, 'o': 68, 'objetivar': 69, 'pasar': 70, 'país': 71, 'pekín': 72, 'perspectivo': 73, 'pib': 74, 'plan': 75, 'popular': 76, 'porcentual': 77, 'presupuestar': 78, 'proyectar': 79, 'puesto': 80, 'punto': 81, 'quinquenal': 82, 'ralentización': 83, 'reformar': 84, 'repasar': 85, 'ruta': 86, 'régimen': 87, 'seguir': 88, 'sesión': 89, 'señalar': 90, 'sufrir': 91, 'tornar': 92, 'urbano': 93, 'xiii': 94, 'y': 95, '\\xa0 ': 96, 'anoche': 97, 'arauco': 98, 'armar': 99, 'arrancar': 100, 'arrestar': 101, 'atacar': 102, 'atentar': 103, 'autoridad': 104, 'biobío': 105, 'camioneta': 106, 'camión': 107, 'carabinero': 108, 'cautelar': 109, 'cañete': 110, 'comenzar': 111, 'cometer': 112, 'comuna': 113, 'concertación': 114, 'conductor': 115, 'convoy': 116, 'curanilahue': 117, 'dedicación': 118, 'delito': 119, 'detención': 120, 'detenido': 121, 'disparar': 122, 'disparo': 123, 'domiciliario': 124, 'efectuar': 125, 'emboscar': 126, 'encapuchar': 127, 'encontrar': 128, 'escopeta': 129, 'etapa': 130, 'exclusivo': 131, 'firmar': 132, 'fiscal': 133, 'forestal': 134, 'formalizar': 135, 'fugar': 136, 'fundir': 137, 'garantir': 138, 'grave': 139, 'h.': 140, 'haber': 141, 'hechizo': 142, 'hecho': 143, 'hombre': 144, 'hospital': 145, 'identificar': 146, 'ilegal': 147, 'imputar': 148, 'incendiario': 149, 'incidente': 150, 'inicial': 151, 'interceptar': 152, 'internar': 153, 'investigación': 154, 'investigar': 155, 'investigativa': 156, 'j.': 157, 'juan': 158, 'judicial': 159, 'jueves': 160, 'juzgar': 161, 'lesionar': 162, 'm.': 163, 'mediodía': 164, 'mesar': 165, 'ministerio': 166, 'miércoles': 167, 'mujer': 168, 'munición': 169, 'nocturno': 170, 'p.': 171, 'parapetar': 172, 'participación': 173, 'patrullaje': 174, 'patrullar': 175, 'permanecer': 176, 'persecutor': 177, 'personar': 178, 'plazo': 179, 'policía': 180, 'portar': 181, 'presentar': 182, 'preventivo': 183, 'prisión': 184, 'procedimiento': 185, 'producir': 186, 'producto': 187, 'protagonizar': 188, 'provincia': 189, 'público': 190, 'quemar': 191, 'realizar': 192, 'receptación': 193, 'rechazar': 194, 'región': 195, 'resultar': 196, 'rural': 197, 'semanal': 198, 'solicitar': 199, 'sujetar': 200, 'sujeto': 201, 'tipo': 202, 'tirúa': 203, 'tribunal': 204, 'vehículo': 205, 'violencia': 206, 'wagemann': 207, 'y.': 208, 'yáñez': 209, 'zona': 210, '  ': 211, '3': 212, 'aprender': 213, 'asignar': 214, 'asistir': 215, 'atención': 216, 'autorización': 217, 'autorizar': 218, 'benjamín': 219, 'básico': 220, 'cambiar': 221, 'caminar': 222, 'cartero': 223, 'casar': 224, 'ciclar': 225, 'clase': 226, 'colegiar': 227, 'constituir': 228, 'contar': 229, 'correr': 230, 'corresponder': 231, 'culminar': 232, 'cursar': 233, 'decidir': 234, 'designar': 235, 'división': 236, 'duoc': 237, 'edad': 238, 'educacional': 239, 'educación': 240, 'educarse': 241, 'efectivamente': 242, 'entrar': 243, 'entregar': 244, 'escoger': 245, 'escolar': 246, 'escuela': 247, 'específico': 248, 'esperar': 249, 'establecimiento': 250, 'estadístico': 251, 'estimar': 252, 'estudiante': 253, 'estudiar': 254, 'estudio': 255, 'examen': 256, 'examinador': 257, 'excepcionalidad': 258, 'facilitar': 259, 'favorecer': 260, 'formar': 261, 'fácil': 262, 'gestión': 263, 'gonzalo': 264, 'gracia': 265, 'herrero': 266, 'infografía': 267, 'informático': 268, 'ingeniería': 269, 'ingresar': 270, 'inscribirse': 271, 'jefe': 272, 'joven': 273, 'libre': 274, 'libro': 275, 'llamativo': 276, 'malo': 277, 'mariano': 278, 'mayor': 279, 'menor': 280, 'mil': 281, 'mineduc': 282, 'mitad': 283, 'muñoz': 284, 'nivelar': 285, 'niño': 286, 'opción': 287, 'optar': 288, 'padre': 289, 'parra': 290, 'particular': 291, 'personal': 292, 'pesar': 293, 'poder': 294, 'preferentemente': 295, 'preparar': 296, 'problema': 297, 'profesor': 298, 'programar': 299, 'prueba': 300, 'querer': 301, 'recalcar': 302, 'rendir': 303, 'responsable': 304, 'semana': 305, 'sentir': 306, 'simce': 307, 'sistema': 308, 'situación': 309, 'temario': 310, 'terminar': 311, 'tomar': 312, 'trámite': 313, 'volver': 314, '°': 315, 'activar': 316, 'afectar': 317, 'agregar': 318, 'altamente': 319, 'américa': 320, 'anomalía': 321, 'basar': 322, 'bebé': 323, 'biológico': 324, 'brotar': 325, 'capar': 326, 'capaz': 327, 'caso': 328, 'causar': 329, 'cell': 330, 'celular': 331, 'cerebral': 332, 'cerebro': 333, 'científico': 334, 'circunstancial': 335, 'claramente': 336, 'codirigir': 337, 'coherente': 338, 'comparación': 339, 'complicación': 340, 'comprobar': 341, 'concluir': 342, 'conocer': 343, 'copiar': 344, 'crucial': 345, 'cráneo': 346, 'célula': 347, 'córtex': 348, 'dañar': 349, 'decena': 350, 'demostrar': 351, 'destruir': 352, 'directamente': 353, 'drástico': 354, 'eficacia': 355, 'experiencia': 356, 'experimental': 357, 'explicar': 358, 'exponer': 359, 'exposición': 360, 'extender': 361, 'fetal': 362, 'feto': 363, 'gen': 364, 'gripe': 365, 'guo': 366, 'hallar': 367, 'hnpcs': 368, 'hopkins': 369, 'humano': 370, 'ileso': 371, 'incrementar': 372, 'indicar': 373, 'infección': 374, 'infectar': 375, 'instituto': 376, 'inusual': 377, 'johns': 378, 'laboratorio': 379, 'latino': 380, 'luchar': 381, 'madre': 382, 'maduro': 383, 'malformación': 384, 'maryland': 385, 'medicamento': 386, 'microcefalia': 387, 'ming': 388, 'morir': 389, 'mosquito': 390, 'nacido': 391, 'necesario': 392, 'neurología': 393, 'neurona': 394, 'neuronal': 395, 'observación': 396, 'oms': 397, 'organización': 398, 'peligroso': 399, 'potencialmente': 400, 'prever': 401, 'probar': 402, 'progenitor': 403, 'proteger': 404, 'publicar': 405, 'recentar': 406, 'recién': 407, 'reducir': 408, 'relativamente': 409, 'replicar': 410, 'resfriar': 411, 'revelar': 412, 'revistar': 413, 'rápidamente': 414, 'salud': 415, 'severo': 416, 'significativo': 417, 'sospechar': 418, 'stem': 419, 'superficial': 420, 'síntoma': 421, 'tejido': 422, 'transmitir': 423, 'trastorno': 424, 'unir': 425, 'viernes': 426, 'virus': 427, 'vulnerable': 428, 'vínculo': 429, 'zika': 430, '-que': 431, '25-': 432, '3d': 433, 'abril': 434, 'abrir': 435, 'abuelo': 436, 'aclarar': 437, 'acompañar': 438, 'acreditar': 439, 'acuñar': 440, 'adjudicar': 441, 'adulto': 442, 'aforar': 443, 'agrupar': 444, 'aislar': 445, 'alejandro': 446, 'aliens': 447, 'allende': 448, 'ambientar': 449, 'ambos': 450, 'américas': 451, 'angelito': 452, 'animachi': 453, 'animación': 454, 'animador': 455, 'animar': 456, 'anunciar': 457, 'apañar': 458, 'aporte': 459, 'apoyar': 460, 'argentino': 461, 'artístico': 462, 'asociación': 463, 'atiempo': 464, 'audiovisual': 465, 'aventurar': 466, 'bernardita': 467, 'boom': 468, 'brindar': 469, 'bromear': 470, 'brothers': 471, 'buscar': 472, 'cadena': 473, 'cambio': 474, 'canal': 475, 'caro': 476, 'cartoon': 477, 'celebración': 478, 'ceppi': 479, 'ceppi.pero': 480, 'cerdo': 481, 'cerrar': 482, 'chavo': 483, 'chespirito': 484, 'chico': 485, 'chile': 486, 'chilemonos': 487, 'chileno': 488, 'chilote': 489, 'chv': 490, 'ciencia': 491, 'cintar': 492, 'clarita': 493, 'cntv': 494, 'cocar': 495, 'cociña': 496, 'colonia': 497, 'comercial': 498, 'compañía': 499, 'complejo': 500, 'completar': 501, 'comprar': 502, 'concordar': 503, 'concursar': 504, 'condicionar': 505, 'confianza': 506, 'conquistar': 507, 'consejo': 508, 'considerar': 509, 'contenido': 510, 'controlar': 511, 'convertir': 512, 'convertirse': 513, 'convocatorio': 514, 'coproducción': 515, 'corfo': 516, 'costoso': 517, 'cristóbal': 518, 'cultura': 519, 'deber': 520, 'desafío': 521, 'destin': 522, 'dibujo': 523, 'diego': 524, 'dignidad': 525, 'diluviar': 526, 'director': 527, 'dirigir': 528, 'disciplinar': 529, 'disfrutar': 530, 'disney': 531, 'distinto': 532, 'dulce': 533, 'efecto': 534, 'ejemplificar': 535, 'elegir': 536, 'emitir': 537, 'encontrarse': 538, 'enfrentar': 539, 'enumerar': 540, 'error': 541, 'erwin': 542, 'escapar': 543, 'escaso': 544, 'estancar': 545, 'estar': 546, 'estrenar': 547, 'existir': 548, 'exitoso': 549, 'extranjero': 550, 'extremo': 551, 'fantástico': 552, 'fenómeno': 553, 'ficción': 554, 'filial': 555, 'filmar': 556, 'finalizar': 557, 'financiamiento': 558, 'flippos': 559, 'fomentar': 560, 'fondo': 561, 'franja': 562, 'fruto': 563, 'fundación': 564, 'futuro': 565, 'fábula': 566, 'ganar': 567, 'gato': 568, 'gente': 569, 'germán': 570, 'globo': 571, 'glot': 572, 'gozar': 573, 'grande': 574, 'gremial': 575, 'gómez': 576, 'gómez.tras': 577, 'habitar': 578, 'hacer': 579, 'hijo': 580, 'historia': 581, 'hito': 582, 'homeless': 583, 'hostal': 584, 'imaginario': 585, 'impartir': 586, 'importante': 587, 'incierto': 588, 'independiente': 589, 'industrialización': 590, 'infantil': 591, 'inspirar': 592, 'interés': 593, 'invertir': 594, 'joaquín': 595, 'jugar': 596, 'lamentar': 597, 'largo': 598, 'largometraje': 599, 'levantar': 600, 'león': 601, 'librar': 602, 'listo': 603, 'llegar': 604, 'llevar': 605, 'lobo': 606, 'lunes': 607, 'lópez': 608, 'mampato': 609, 'mangar': 610, 'mar': 611, 'marciano': 612, 'mascota': 613, 'matilde': 614, 'mediatización': 615, 'mejorar': 616, 'miedo': 617, 'mirar': 618, 'moderno': 619, 'morrison': 620, 'mostrar': 621, 'motion': 622, 'muelín': 623, 'mundo': 624, 'mágico': 625, 'más.el': 626, 'nahuel': 627, 'naufragar': 628, 'necesitar': 629, 'netflix': 630, 'network': 631, 'nui': 632, 'nulo': 633, 'objetar': 634, 'obtener': 635, 'ogú': 636, 'ojeda': 637, 'ojo': 638, 'oportunidad': 639, 'organismo': 640, 'origen': 641, 'osar': 642, 'oscar': 643, 'pakapaka': 644, 'panorama': 645, 'pantalla': 646, 'papel': 647, 'papelucho': 648, 'parecer': 649, 'paso': 650, 'patrocinio': 651, 'película': 652, 'pensar': 653, 'perlita': 654, 'perpetuar': 655, 'pescador': 656, 'peso': 657, 'pirata': 658, 'planeta': 659, 'plataforma': 660, 'poner': 661, 'preguntar': 662, 'premiar': 663, 'preproducción': 664, 'presentarse': 665, 'presidente': 666, 'producción': 667, 'produciéndose': 668, 'productor': 669, 'profesionalización': 670, 'profundar': 671, 'programación': 672, 'promediar': 673, 'propuesta': 674, 'proyecto': 675, 'puerto': 676, 'pulentos': 677, 'punkrobot': 678, 'pájaro': 679, 'quedar': 680, 'rapar': 681, 'recordar': 682, 'recurso': 683, 'remontar': 684, 'resonancia': 685, 'rico': 686, 'roca': 687, 'rojo': 688, 'salvador': 689, 'seguramente': 690, 'sellar': 691, 'semestre': 692, 'separar': 693, 'sepultar': 694, 'seriar': 695, 'serie': 696, 'silenciar': 697, 'similar': 698, 'simpson': 699, 'softwares': 700, 'soler': 701, 'soñar': 702, 'stop': 703, 'studio': 704, 'studios': 705, 'suerte': 706, 'sumir': 707, 'tarugo': 708, 'televisión': 709, 'temer': 710, 'temporada': 711, 'terminarla': 712, 'tevito': 713, 'tormenta': 714, 'tortuga': 715, 'trabajar': 716, 'transmisión': 717, 'transmitirse': 718, 'tv': 719, 'tvn': 720, 'técnico': 721, 'título': 722, 'universidad': 723, 'utilizar': 724, 'vacación': 725, 'vagabundo': 726, 'valparaíso': 727, 'venir': 728, 'ver': 729, 'verdura': 730, 'villa': 731, 'visitar': 732, 'vivir': 733, 'walt': 734, 'zumbástico': 735, 'álvaro': 736, 'éxito': 737, 'ó': 738, 'único': 739}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(noticias)\n",
    "print(dictionary.token2id)\n",
    "#asigna un id por cada palabra del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformamos el dataset en un representacion vectorial tipo bag of word\n",
    "dataset_vectorized = [dictionary.doc2bow(noticia) for noticia in noticias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (2, 8), (14, 1), (28, 3), (51, 1), (65, 1), (68, 3), (71, 1), (95, 11), (96, 16), (102, 2), (111, 1), (146, 1), (154, 1), (182, 3), (186, 1), (196, 5), (202, 3), (210, 1), (211, 23), (224, 1), (254, 4), (255, 1), (261, 1), (269, 1), (298, 1), (300, 1), (316, 1), (317, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1), (328, 1), (329, 2), (330, 2), (331, 1), (332, 1), (333, 3), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 10), (348, 3), (349, 1), (350, 1), (351, 1), (352, 1), (353, 1), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (363, 3), (364, 1), (365, 1), (366, 1), (367, 1), (368, 3), (369, 1), (370, 4), (371, 1), (372, 1), (373, 1), (374, 1), (375, 3), (376, 2), (377, 1), (378, 1), (379, 3), (380, 1), (381, 1), (382, 1), (383, 1), (384, 1), (385, 1), (386, 1), (387, 4), (388, 2), (389, 1), (390, 1), (391, 1), (392, 1), (393, 1), (394, 2), (395, 1), (396, 1), (397, 1), (398, 1), (399, 1), (400, 1), (401, 1), (402, 1), (403, 1), (404, 1), (405, 2), (406, 1), (407, 1), (408, 2), (409, 1), (410, 1), (411, 1), (412, 1), (413, 1), (414, 1), (415, 1), (416, 2), (417, 1), (418, 1), (419, 1), (420, 1), (421, 1), (422, 1), (423, 1), (424, 1), (425, 1), (426, 1), (427, 10), (428, 2), (429, 2), (430, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_vectorized[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4 Representación vectorial (Gensim): Bag of words (con ponderación TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir el BOW en una representación TF-IDF\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(dataset_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 0.024765875947776374),\n",
       " (28, 0.04142040377269099),\n",
       " (51, 0.024765875947776374),\n",
       " (65, 0.024765875947776374),\n",
       " (68, 0.04142040377269099),\n",
       " (71, 0.013806801257563662),\n",
       " (96, 0.09649942435350073),\n",
       " (102, 0.04953175189555275),\n",
       " (111, 0.0060312140220937956),\n",
       " (146, 0.024765875947776374),\n",
       " (154, 0.024765875947776374),\n",
       " (182, 0.07429762784332912),\n",
       " (186, 0.013806801257563662),\n",
       " (196, 0.0690340062878183),\n",
       " (202, 0.04142040377269099),\n",
       " (210, 0.024765875947776374),\n",
       " (211, 0.31755642892396424),\n",
       " (224, 0.013806801257563662),\n",
       " (254, 0.0990635037911055),\n",
       " (255, 0.013806801257563662),\n",
       " (261, 0.024765875947776374),\n",
       " (269, 0.024765875947776374),\n",
       " (298, 0.024765875947776374),\n",
       " (300, 0.024765875947776374),\n",
       " (316, 0.04350053787345895),\n",
       " (317, 0.04350053787345895),\n",
       " (318, 0.024765875947776374),\n",
       " (319, 0.04350053787345895),\n",
       " (320, 0.04350053787345895),\n",
       " (321, 0.04350053787345895),\n",
       " (322, 0.04350053787345895),\n",
       " (323, 0.04350053787345895),\n",
       " (324, 0.04350053787345895),\n",
       " (325, 0.04350053787345895),\n",
       " (326, 0.04350053787345895),\n",
       " (327, 0.04350053787345895),\n",
       " (328, 0.04350053787345895),\n",
       " (329, 0.0870010757469179),\n",
       " (330, 0.0870010757469179),\n",
       " (331, 0.04350053787345895),\n",
       " (332, 0.04350053787345895),\n",
       " (333, 0.13050161362037685),\n",
       " (334, 0.04350053787345895),\n",
       " (335, 0.04350053787345895),\n",
       " (336, 0.04350053787345895),\n",
       " (337, 0.04350053787345895),\n",
       " (338, 0.04350053787345895),\n",
       " (339, 0.04350053787345895),\n",
       " (340, 0.04350053787345895),\n",
       " (341, 0.04350053787345895),\n",
       " (342, 0.024765875947776374),\n",
       " (343, 0.024765875947776374),\n",
       " (344, 0.04350053787345895),\n",
       " (345, 0.04350053787345895),\n",
       " (346, 0.04350053787345895),\n",
       " (347, 0.4350053787345895),\n",
       " (348, 0.13050161362037685),\n",
       " (349, 0.04350053787345895),\n",
       " (350, 0.024765875947776374),\n",
       " (351, 0.04350053787345895),\n",
       " (352, 0.04350053787345895),\n",
       " (353, 0.04350053787345895),\n",
       " (354, 0.04350053787345895),\n",
       " (355, 0.04350053787345895),\n",
       " (356, 0.04350053787345895),\n",
       " (357, 0.04350053787345895),\n",
       " (358, 0.024765875947776374),\n",
       " (359, 0.04350053787345895),\n",
       " (360, 0.04350053787345895),\n",
       " (361, 0.04350053787345895),\n",
       " (362, 0.04350053787345895),\n",
       " (363, 0.13050161362037685),\n",
       " (364, 0.04350053787345895),\n",
       " (365, 0.04350053787345895),\n",
       " (366, 0.04350053787345895),\n",
       " (367, 0.04350053787345895),\n",
       " (368, 0.13050161362037685),\n",
       " (369, 0.04350053787345895),\n",
       " (370, 0.1740021514938358),\n",
       " (371, 0.04350053787345895),\n",
       " (372, 0.04350053787345895),\n",
       " (373, 0.04350053787345895),\n",
       " (374, 0.04350053787345895),\n",
       " (375, 0.13050161362037685),\n",
       " (376, 0.0870010757469179),\n",
       " (377, 0.04350053787345895),\n",
       " (378, 0.04350053787345895),\n",
       " (379, 0.13050161362037685),\n",
       " (380, 0.04350053787345895),\n",
       " (381, 0.04350053787345895),\n",
       " (382, 0.04350053787345895),\n",
       " (383, 0.04350053787345895),\n",
       " (384, 0.04350053787345895),\n",
       " (385, 0.04350053787345895),\n",
       " (386, 0.04350053787345895),\n",
       " (387, 0.1740021514938358),\n",
       " (388, 0.0870010757469179),\n",
       " (389, 0.04350053787345895),\n",
       " (390, 0.04350053787345895),\n",
       " (391, 0.04350053787345895),\n",
       " (392, 0.04350053787345895),\n",
       " (393, 0.04350053787345895),\n",
       " (394, 0.0870010757469179),\n",
       " (395, 0.04350053787345895),\n",
       " (396, 0.04350053787345895),\n",
       " (397, 0.04350053787345895),\n",
       " (398, 0.04350053787345895),\n",
       " (399, 0.04350053787345895),\n",
       " (400, 0.04350053787345895),\n",
       " (401, 0.04350053787345895),\n",
       " (402, 0.04350053787345895),\n",
       " (403, 0.04350053787345895),\n",
       " (404, 0.04350053787345895),\n",
       " (405, 0.0870010757469179),\n",
       " (406, 0.04350053787345895),\n",
       " (407, 0.04350053787345895),\n",
       " (408, 0.0870010757469179),\n",
       " (409, 0.04350053787345895),\n",
       " (410, 0.04350053787345895),\n",
       " (411, 0.04350053787345895),\n",
       " (412, 0.04350053787345895),\n",
       " (413, 0.04350053787345895),\n",
       " (414, 0.04350053787345895),\n",
       " (415, 0.04350053787345895),\n",
       " (416, 0.0870010757469179),\n",
       " (417, 0.04350053787345895),\n",
       " (418, 0.04350053787345895),\n",
       " (419, 0.04350053787345895),\n",
       " (420, 0.04350053787345895),\n",
       " (421, 0.04350053787345895),\n",
       " (422, 0.04350053787345895),\n",
       " (423, 0.024765875947776374),\n",
       " (424, 0.04350053787345895),\n",
       " (425, 0.04350053787345895),\n",
       " (426, 0.04350053787345895),\n",
       " (427, 0.4350053787345895),\n",
       " (428, 0.0870010757469179),\n",
       " (429, 0.0870010757469179),\n",
       " (430, 0.2610032272407537)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[dataset_vectorized][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Aplicación en Análisis de sentimientos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos importando las librerías que necesitaremos para esta tarea. Ya hemos importado spaCy, pero también necesitaremos pandas y scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Dataset\n",
    "Vamos a usar un conjunto de datos real: un conjunto de reseñas de productos de Amazon Alexa. Este conjunto de datos viene como un archivo separado por tabulaciones (.tsv). Tiene cinco columnas: \n",
    "- __rating__: se refiere a la calificación que cada usuario dio a Alexa (de 0 a 5). \n",
    "- __fecha__: fecha de la reseña\n",
    "- __variación__: describe el modelo de producto Alexa que el usuario comentó.\n",
    "- __verified_reviews__: contiene el texto del comentario.\n",
    "- __feedback__: contiene un label, 0 o 1, que indica el sentimiento general negativo (0) o positivo (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TSV file\n",
    "df_amazon = pd.read_csv (\"amazon_alexa.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Heather Gray Fabric</td>\n",
       "      <td>I received the echo as a gift. I needed anothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Sandstone Fabric</td>\n",
       "      <td>Without having a cellphone, I cannot use many ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I think this is the 5th one I've purchased. I'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Heather Gray Fabric</td>\n",
       "      <td>looks great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Heather Gray Fabric</td>\n",
       "      <td>Love it! I’ve listened to songs I haven’t hear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I sent it to my 85 year old Dad, and he talks ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I love it! Learning knew things with it eveyda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Oak Finish</td>\n",
       "      <td>I purchased this for my mother who is having k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love, Love, Love!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Oak Finish</td>\n",
       "      <td>Just what I expected....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Heather Gray Fabric</td>\n",
       "      <td>I love it, wife hates it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Heather Gray Fabric</td>\n",
       "      <td>Really happy with this purchase.  Great speake...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Heather Gray Fabric</td>\n",
       "      <td>We have only been using Alexa for a couple of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>We love the size of the 2nd generation echo. S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Sandstone Fabric</td>\n",
       "      <td>I liked the original Echo. This is the same bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love the Echo and how good the music sounds pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>We love Alexa! We use her to play music, play ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Heather Gray Fabric</td>\n",
       "      <td>Have only had it set up for a few days. Still ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I love it. It plays my sleep sounds immediatel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Sandstone Fabric</td>\n",
       "      <td>I got a second unit for the bedroom, I was exp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Sandstone Fabric</td>\n",
       "      <td>Amazing product</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I love my Echo. It's easy to operate, loads of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Sounds great!! Love them!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Fun item to play with and get used to using.  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Just like the other one</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>I like the hands free operation vs the Tap. We...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>3</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>I dislike that it confuses my requests all the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Love my Alexa! Actually have 3 throughout the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>This product is easy to use and very entertain...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>works great but speaker is not the good for mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>Outstanding product - easy to use.  works great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>We have six of these throughout our home and t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Use the product for music and it’s great!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Easy to set-up and to use.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>It works great!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>I like having more Alexa devices in my house a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>I loved it does exactly what it says</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>I used it to control my smart home devices. Wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Very convenient</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>Este producto llegó y a la semana se quedó sin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>Easy to set up Ready to use in minutes.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>Barry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>3</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>My three year old loves it.  Good for doing ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Awesome device wish I bought one ages ago.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>love it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Perfect for kids, adults and everyone in betwe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Listening to music, searching locations, check...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>I do love these things, i have them running my...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>Only complaint I have is that the sound qualit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3149</th>\n",
       "      <td>4</td>\n",
       "      <td>29-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating       date             variation  \\\n",
       "0          5  31-Jul-18      Charcoal Fabric    \n",
       "1          5  31-Jul-18      Charcoal Fabric    \n",
       "2          4  31-Jul-18        Walnut Finish    \n",
       "3          5  31-Jul-18      Charcoal Fabric    \n",
       "4          5  31-Jul-18      Charcoal Fabric    \n",
       "5          5  31-Jul-18  Heather Gray Fabric    \n",
       "6          3  31-Jul-18     Sandstone Fabric    \n",
       "7          5  31-Jul-18      Charcoal Fabric    \n",
       "8          5  30-Jul-18  Heather Gray Fabric    \n",
       "9          5  30-Jul-18  Heather Gray Fabric    \n",
       "10         5  30-Jul-18      Charcoal Fabric    \n",
       "11         5  30-Jul-18      Charcoal Fabric    \n",
       "12         5  30-Jul-18           Oak Finish    \n",
       "13         5  30-Jul-18      Charcoal Fabric    \n",
       "14         5  30-Jul-18           Oak Finish    \n",
       "15         5  30-Jul-18  Heather Gray Fabric    \n",
       "16         5  30-Jul-18  Heather Gray Fabric    \n",
       "17         5  30-Jul-18  Heather Gray Fabric    \n",
       "18         5  30-Jul-18      Charcoal Fabric    \n",
       "19         4  30-Jul-18     Sandstone Fabric    \n",
       "20         5  30-Jul-18      Charcoal Fabric    \n",
       "21         5  30-Jul-18      Charcoal Fabric    \n",
       "22         4  30-Jul-18  Heather Gray Fabric    \n",
       "23         5  30-Jul-18      Charcoal Fabric    \n",
       "24         3  30-Jul-18     Sandstone Fabric    \n",
       "25         5  30-Jul-18     Sandstone Fabric    \n",
       "26         5  30-Jul-18      Charcoal Fabric    \n",
       "27         5  30-Jul-18      Charcoal Fabric    \n",
       "28         4  30-Jul-18      Charcoal Fabric    \n",
       "29         5  30-Jul-18      Charcoal Fabric    \n",
       "...      ...        ...                   ...   \n",
       "3120       5  30-Jul-18            Black  Dot   \n",
       "3121       5  30-Jul-18            Black  Dot   \n",
       "3122       3  30-Jul-18            Black  Dot   \n",
       "3123       4  30-Jul-18            Black  Dot   \n",
       "3124       5  30-Jul-18            Black  Dot   \n",
       "3125       4  30-Jul-18            Black  Dot   \n",
       "3126       5  30-Jul-18            Black  Dot   \n",
       "3127       4  30-Jul-18            Black  Dot   \n",
       "3128       5  30-Jul-18            White  Dot   \n",
       "3129       4  30-Jul-18            White  Dot   \n",
       "3130       5  30-Jul-18            Black  Dot   \n",
       "3131       5  30-Jul-18            Black  Dot   \n",
       "3132       5  30-Jul-18            Black  Dot   \n",
       "3133       4  30-Jul-18            White  Dot   \n",
       "3134       5  30-Jul-18            Black  Dot   \n",
       "3135       5  30-Jul-18            White  Dot   \n",
       "3136       4  30-Jul-18            Black  Dot   \n",
       "3137       5  30-Jul-18            Black  Dot   \n",
       "3138       5  30-Jul-18            White  Dot   \n",
       "3139       5  30-Jul-18            White  Dot   \n",
       "3140       4  30-Jul-18            White  Dot   \n",
       "3141       3  30-Jul-18            Black  Dot   \n",
       "3142       4  30-Jul-18            White  Dot   \n",
       "3143       5  30-Jul-18            Black  Dot   \n",
       "3144       5  30-Jul-18            Black  Dot   \n",
       "3145       5  30-Jul-18            Black  Dot   \n",
       "3146       5  30-Jul-18            Black  Dot   \n",
       "3147       5  30-Jul-18            Black  Dot   \n",
       "3148       5  30-Jul-18            White  Dot   \n",
       "3149       4  29-Jul-18            Black  Dot   \n",
       "\n",
       "                                       verified_reviews  feedback  \n",
       "0                                         Love my Echo!         1  \n",
       "1                                             Loved it!         1  \n",
       "2     Sometimes while playing a game, you can answer...         1  \n",
       "3     I have had a lot of fun with this thing. My 4 ...         1  \n",
       "4                                                 Music         1  \n",
       "5     I received the echo as a gift. I needed anothe...         1  \n",
       "6     Without having a cellphone, I cannot use many ...         1  \n",
       "7     I think this is the 5th one I've purchased. I'...         1  \n",
       "8                                           looks great         1  \n",
       "9     Love it! I’ve listened to songs I haven’t hear...         1  \n",
       "10    I sent it to my 85 year old Dad, and he talks ...         1  \n",
       "11    I love it! Learning knew things with it eveyda...         1  \n",
       "12    I purchased this for my mother who is having k...         1  \n",
       "13                                   Love, Love, Love!!         1  \n",
       "14                             Just what I expected....         1  \n",
       "15                            I love it, wife hates it.         1  \n",
       "16    Really happy with this purchase.  Great speake...         1  \n",
       "17    We have only been using Alexa for a couple of ...         1  \n",
       "18    We love the size of the 2nd generation echo. S...         1  \n",
       "19    I liked the original Echo. This is the same bu...         1  \n",
       "20    Love the Echo and how good the music sounds pl...         1  \n",
       "21    We love Alexa! We use her to play music, play ...         1  \n",
       "22    Have only had it set up for a few days. Still ...         1  \n",
       "23    I love it. It plays my sleep sounds immediatel...         1  \n",
       "24    I got a second unit for the bedroom, I was exp...         1  \n",
       "25                                      Amazing product         1  \n",
       "26    I love my Echo. It's easy to operate, loads of...         1  \n",
       "27                            Sounds great!! Love them!         1  \n",
       "28    Fun item to play with and get used to using.  ...         1  \n",
       "29                              Just like the other one         1  \n",
       "...                                                 ...       ...  \n",
       "3120                                                            1  \n",
       "3121  I like the hands free operation vs the Tap. We...         1  \n",
       "3122  I dislike that it confuses my requests all the...         1  \n",
       "3123                                                            1  \n",
       "3124  Love my Alexa! Actually have 3 throughout the ...         1  \n",
       "3125  This product is easy to use and very entertain...         1  \n",
       "3126                                                            1  \n",
       "3127  works great but speaker is not the good for mu...         1  \n",
       "3128    Outstanding product - easy to use.  works great         1  \n",
       "3129  We have six of these throughout our home and t...         1  \n",
       "3130          Use the product for music and it’s great!         1  \n",
       "3131                         Easy to set-up and to use.         1  \n",
       "3132                                   It works great!!         1  \n",
       "3133  I like having more Alexa devices in my house a...         1  \n",
       "3134                                         PHENOMENAL         1  \n",
       "3135               I loved it does exactly what it says         1  \n",
       "3136  I used it to control my smart home devices. Wo...         1  \n",
       "3137                                    Very convenient         1  \n",
       "3138  Este producto llegó y a la semana se quedó sin...         1  \n",
       "3139            Easy to set up Ready to use in minutes.         1  \n",
       "3140                                              Barry         1  \n",
       "3141                                                            1  \n",
       "3142  My three year old loves it.  Good for doing ba...         1  \n",
       "3143         Awesome device wish I bought one ages ago.         1  \n",
       "3144                                            love it         1  \n",
       "3145  Perfect for kids, adults and everyone in betwe...         1  \n",
       "3146  Listening to music, searching locations, check...         1  \n",
       "3147  I do love these things, i have them running my...         1  \n",
       "3148  Only complaint I have is that the sound qualit...         1  \n",
       "3149                                               Good         1  \n",
       "\n",
       "[3150 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 records\n",
    "df_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataframe\n",
    "df_amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      "rating              3150 non-null int64\n",
      "date                3150 non-null object\n",
      "variation           3150 non-null object\n",
      "verified_reviews    3150 non-null object\n",
      "feedback            3150 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 123.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# View data information\n",
    "df_amazon.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2893\n",
       "0     257\n",
       "Name: feedback, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedback Value count\n",
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Preprocesamientos y representación vectorial:\n",
    "\n",
    "Crearemos una función personalizada <code>spacy_tokenizer()</code> que acepta una frase como entrada y la procesa en tokens, realizando lemmatización, minúsculas y eliminando palabras stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "stop_words=\"\"\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorización de los textos en BoW o TF-IDF, con scikit-learn\n",
    "\n",
    "Podemos generar una matriz BoW para nuestros datos de texto usando la clase <code>CountVectorizer</code> de scikit-learn. En el código de abajo, le decimos a CountVectorizer que use la función personalizada spacy_tokenizer que construimos como su tokenizer, y que defina el rango de ngramo que queremos.\n",
    "\n",
    "Los N-gramos son combinaciones de palabras adyacentes en un texto dado, donde _n_ es el número de palabras que se incluyen en las fichas. Por ejemplo, en la frase \"¿Quién ganará la Copa del Mundo de fútbol en 2022? Bigramas sería una secuencia de dos palabras contiguas como \"quién ganará\", \"ganará la\", y así sucesivamente. Así que el parámetro ngram_range que usaremos en el código de abajo establece los límites inferior y superior de nuestros ngramas (usaremos unigramas). Entonces asignaremos los ngramas a bow_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f4a0d030a70>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "bow_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podriamos también transformar los textos en vectores para tener los pesos TF-IDF de cada palabra en cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partición de los datos en conjuntos de entrenamiento y test para entrenar y evaluar un modelo predictivo\n",
    "\n",
    "Usaremos la mitad de nuestro conjunto de datos como nuestro conjunto de entrenamiento, que incluirá las respuestas correctas. Luego probaremos nuestro modelo usando la otra mitad del conjunto de datos sin darle las respuestas, para ver con qué precisión funciona.\n",
    "\n",
    "Convenientemente, scikit-learn nos da una función incorporada para hacer esto: train_test_split(). Sólo necesitamos decirle el conjunto de características que queremos que se divida (X), las etiquetas contra las que queremos que se realice la prueba (ylabels), y el tamaño que queremos usar para el conjunto de pruebas (representado como un porcentaje en forma decimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de un pipeline y generación del modelo\n",
    "\n",
    "Es el momento de construir nuestro modelo predictivo. Empezaremos importando el módulo LogisticRegression y creando un objeto clasificador LogisticRegression.\n",
    "\n",
    "__Para revisar sobre qué aprende y cómo aprendre el algorítmo de regresión logística: [slides](https://docs.google.com/presentation/d/11O3ud6ywHuaro6OemhyeH07nuJtdc4ybMuTJicnMnm8/edit?usp=sharing)__\n",
    "\n",
    "Luego, crearemos un pipeline de procesamiento con dos componentes: un vectorizador y algoritmo de clasificación basado en la regresión logística. El vectorizador utiliza preprocesamientos (spacy) y vectorización (scikit-learn) para crear una matriz para representar nuestros textos.\n",
    "\n",
    "Una vez que se construya este pipeline, se aprende el modelo predictivo llamando el método <code>fit()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f4a0d030a70>)),\n",
       "                ('regression-ML', LogisticRegression())])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([('preprocessing', bow_vector),\n",
    "                 ('regression-ML', modelLR)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Evaluación del modelo\n",
    "\n",
    "Podemos evaluar el rendimiento de nuestro modelo usando el módulo de métricas de scikit-learn. Ahora que hemos entrenado nuestro modelo, pondremos nuestros datos de prueba a disposición para hacer predicciones. Luego usaremos varias funciones del módulo de métricas para ver la exactitud, precisión y recall de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "Logistic Regression Accuracy: 0.926984126984127\n",
      "Logistic Regression Precision: 0.9382879893828799\n",
      "Logistic Regression Recall: 0.9846796657381616\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "print(predicted)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  46   93]\n",
      " [  22 1414]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.33      0.44       139\n",
      "           1       0.94      0.98      0.96      1436\n",
      "\n",
      "    accuracy                           0.93      1575\n",
      "   macro avg       0.81      0.66      0.70      1575\n",
      "weighted avg       0.92      0.93      0.92      1575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluación del rendimiento del clasificador\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted)\n",
    "print(confusion_matrix)\n",
    "#Print de la matriz de confusión\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, model, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(model.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 best: \n",
      "(-1.523500918228262, 'not')\n",
      "(-1.4895458665670425, 'cheap')\n",
      "(-1.209826179658307, 'terrible')\n",
      "(-1.0207365966052975, 'when')\n",
      "(-0.9577452896567407, 'order')\n",
      "(-0.9276789758069817, 'useless')\n",
      "(-0.9012959935901782, 'after')\n",
      "(-0.8881799706271553, 'return')\n",
      "(-0.8827898726746258, 'no')\n",
      "(-0.8475170799055632, 'difficult')\n",
      "(-0.8459853835822712, 'apple')\n",
      "(-0.8377438842152846, 'meh')\n",
      "(-0.8337022081831309, 'twice')\n",
      "(-0.7923554245400001, 'intrusive')\n",
      "(-0.7767690207470604, 'stopped')\n",
      "(-0.7766766749271931, 'returned')\n",
      "(-0.7406681322876024, 'much')\n",
      "(-0.7343765148322142, 'disconnects')\n",
      "(-0.7290412942041861, 'often')\n",
      "(-0.7185390114173066, 'spent')\n",
      "Class 2 best: \n",
      "(2.0906839677153655, 'love')\n",
      "(1.5934620981934158, 'great')\n",
      "(1.2394830608287888, 'easy')\n",
      "(0.9250544049039695, 'works')\n",
      "(0.8672128880120398, 'but')\n",
      "(0.8182692072977023, 'like')\n",
      "(0.7543535892365358, 'fun')\n",
      "(0.7513033163023595, 'good')\n",
      "(0.6695625416659539, 'cool')\n",
      "(0.6536625226090867, '...')\n",
      "(0.6237753402684684, 'my')\n",
      "(0.5963207629392794, 'she')\n",
      "(0.5946585435583304, 'awesome')\n",
      "(0.5860852018460277, 'perfect')\n",
      "(0.5858700131923287, 'excellent')\n",
      "(0.554380603586611, 'why')\n",
      "(0.5423599127688297, 'tv')\n",
      "(0.5418075666324919, 'new')\n",
      "(0.539180880259775, 'know')\n",
      "(0.5320954095825057, 'can')\n"
     ]
    }
   ],
   "source": [
    "printNMostInformative(bow_vector, modelLR, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Ejercicio: Análisis de sentimientos en español\n",
    "\n",
    "Análisis de sentimientos de los tweets en español sobre aerolineas: Analizar cómo los viajeros expresaron sus sentimientos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>926419989107798016</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Nov 03 12:05:12 +0000 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934854385577943041</th>\n",
       "      <td>neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Nov 26 18:40:28 +0000 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945318406441635840</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Dec 25 15:40:45 +0000 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927540721296568320</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Nov 06 14:18:35 +0000 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947965901332197376</th>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 01 23:00:57 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buenos Aires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   airline_sentiment  is_reply  reply_count  retweet_count  \\\n",
       "tweet_id                                                                     \n",
       "926419989107798016           neutral     False            0              0   \n",
       "934854385577943041           neutral      True            0              0   \n",
       "945318406441635840          negative     False            0              0   \n",
       "927540721296568320          negative     False            0              0   \n",
       "947965901332197376          positive      True            0              0   \n",
       "\n",
       "                                                                 text  \\\n",
       "tweet_id                                                                \n",
       "926419989107798016  Trabajar en #Ryanair como #TMA: https://t.co/r...   \n",
       "934854385577943041  @Iberia @FIONAFERRER Cuando gusten en Cancún s...   \n",
       "945318406441635840  Sabiais que @Iberia te trata muy bien en santi...   \n",
       "927540721296568320  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...   \n",
       "947965901332197376  @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...   \n",
       "\n",
       "                   tweet_coord                   tweet_created tweet_location  \\\n",
       "tweet_id                                                                        \n",
       "926419989107798016         NaN  Fri Nov 03 12:05:12 +0000 2017            NaN   \n",
       "934854385577943041         NaN  Sun Nov 26 18:40:28 +0000 2017            NaN   \n",
       "945318406441635840         NaN  Mon Dec 25 15:40:45 +0000 2017            NaN   \n",
       "927540721296568320         NaN  Mon Nov 06 14:18:35 +0000 2017            NaN   \n",
       "947965901332197376         NaN  Mon Jan 01 23:00:57 +0000 2018            NaN   \n",
       "\n",
       "                                 user_timezone  \n",
       "tweet_id                                        \n",
       "926419989107798016                      Madrid  \n",
       "934854385577943041                 Mexico City  \n",
       "945318406441635840                      Madrid  \n",
       "927540721296568320  Pacific Time (US & Canada)  \n",
       "947965901332197376                Buenos Aires  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_public.csv', encoding='utf-8', index_col='tweet_id')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
